{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧰 modules overview: retrieval\n",
    "\n",
    "*Utilities for \"chatting your data\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\", \"src\"))) # hack for importing src/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the `retrieval` module exports `Retrievers`. These classes are essentially wrappers over a data source that internally extract meaningful chunks from the ingest data before storing it. They can then provide useful higher-level search functions over that data. This pattern is meant to accommodate the \"chat my data\" use case. As we'll show later with `PythonRetriever`, this pattern neatly abstracts the operations of parsing python (extraction), storing it, and making it queryable. \n",
    "\n",
    "To start, we'll look at the basic `DocumentRetriever`. Let's give it an in memory `Chroma` db (our own light-weight wrapper that implements our interface), index a few documents, and then search on them with natural language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "975c2343ec3048d888850ba274167dbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding documents to store:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a77b1228f774ffe824000d8aae3f117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding documents to store:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ea4935659a84b86bf8da686aeab7756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding documents to store:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[DocumentExtractorResult(document='exemple trois', metadata={'extracted_at': 1685649666})]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from db import Chroma\n",
    "\n",
    "from retrieval import DocumentRetriever\n",
    "\n",
    "db = Chroma('basic-doc', \"/Users/jmn/chroma/retrieval-module-doc\")\n",
    "\n",
    "retriever = DocumentRetriever(db)\n",
    "\n",
    "retriever.load(\"example one\")\n",
    "retriever.load(\"ejemplo dos\")\n",
    "retriever.load(\"exemple trois\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searches that have nothing in common with the exact document text produce the expected results - neat!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QueryResult(_id='c89e2f87-dcad-43e4-8087-cf7e961ecb6b', document='exemple trois', metadata={'extracted_at': 1685649666}, distance=0.17967736721038818)]\n",
      "[QueryResult(_id='239f775e-a6a7-44d3-8eed-b0dcbcecac9b', document='ejemplo dos', metadata={'extracted_at': 1685649666}, distance=0.1953725814819336)]\n",
      "[QueryResult(_id='0f0dad35-7b36-4400-a447-64734778cc5d', document='example one', metadata={'extracted_at': 1685649666}, distance=0.17287689447402954)]\n",
      "[QueryResult(_id='0f0dad35-7b36-4400-a447-64734778cc5d', document='example one', metadata={'extracted_at': 1685649666}, distance=0.1942535638809204)]\n"
     ]
    }
   ],
   "source": [
    "print(retriever.query(\"written in french\", max_results=1))\n",
    "print(retriever.query(\"written in spanish\", max_results=1))\n",
    "print(retriever.query(\"first item\", max_results=1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🐍 Parsing code and getting metadata with `PythonExtractor`\n",
    "\n",
    "Before looking at the `PythonRetriever` exported by the top-level `retrieval` module, let's look at the more internal `PythonExtractor`. This is a configurable utility class that parses Python code and extracts meaningful chunks along with their associated metadata. The metadata includes where to locate the chunk (the file path and line number) and more. The result is something that is hopefully more useful for embedding to facilitate code search and \"chatting your data\" in general. As a bonus, the `PythonRetriever` module is able to provide useful higher-level search methods because of the metadata produced by the extractor.\n",
    "\n",
    "The default configuration of `PythonExtractor` produces a lot of redundant chunks. The chunks extracted include: entire modules, classes, functions (class methods each count as their own function), block comments (for docstrings), function/method calls, and variable assignments. So a docstring in a method, for instance, will get extracted many times. This is by design! However in your own production code, you can easily customize exactly what you want to extract by using the DI patterns (through constructor params) that are used in `retrieval` (and throughout gpt-toolbox!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diving into /Users/jmn/Projects/gpt-toolbox/src...\n",
      "done. total items extracted: 613\n",
      "{\n",
      "  \"extracted_at\": 1685649668,\n",
      "  \"node_type\": \"method\",\n",
      "  \"output_type\": \"code\",\n",
      "  \"node_name\": \"search_comments\",\n",
      "  \"lineno\": 25,\n",
      "  \"loc\": 2,\n",
      "  \"lloc\": 3,\n",
      "  \"sloc\": 2,\n",
      "  \"file_name\": \"python_retriever.py\",\n",
      "  \"file_path\": \"/Users/jmn/Projects/gpt-toolbox/src/retrieval/python_retriever.py\",\n",
      "  \"last_modified_time\": 1685135687\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from retrieval.extract import PythonProjectExtractor\n",
    "\n",
    "extractor = PythonProjectExtractor()\n",
    "\n",
    "target_dir = os.path.join(os.getcwd(), \"..\", \"src\") # extract our own src/\n",
    "\n",
    "print(f\"diving into {os.path.abspath(target_dir)}...\")\n",
    "\n",
    "items = extractor.extract(target_dir)\n",
    "\n",
    "print('done. total items extracted:', len(items))\n",
    "\n",
    "print(json.dumps(items[405].metadata, indent=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 📜 Indexing with `Chroma`\n",
    "\n",
    "**Warning: The following operations cost money! (~$0.02)**\n",
    "\n",
    "Now we'll step out of `retrieval` for a second and manually index everything we just extracted into Chroma to see how that looks (`retrievers` do this interally)\n",
    "\n",
    "Even though we are indexing a lot of redundant text (as mentioned before), it should only cost a few pennies (ada-002 is currently 1/50th the cost of gpt3.5, and there's no completion to worry about)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total documents in store: 0\n"
     ]
    }
   ],
   "source": [
    "db = Chroma('python-example-1', \"/Users/jmn/chroma/retrieval-module-doc\")\n",
    "print('total documents in store:', db.collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ae03ac68e894e808f8deb923f9ef582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding documents to store:   0%|          | 0/613 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total documents in store: 621\n"
     ]
    }
   ],
   "source": [
    "for item in tqdm(items, desc=\"Adding documents to store\"):\n",
    "    db.add_document(item.document, item.metadata)\n",
    "\n",
    "db.client.persist() # only necessary in notebook context\n",
    "\n",
    "print('total documents in store:', db.collection.count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🔎 Searching\n",
    "\n",
    " With the chunks embedded, you can search the code using natural language or with exact references to symbols.  You can also filter on metadata for special searches, e.g. calls to a function, or searching only within docstrings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  \"/Users/jmn/Projects/gpt-toolbox/src/agents/web/agent.py:28 prompt (method-code)\",\n",
      "  \"/Users/jmn/Projects/gpt-toolbox/src/agents/few_shot/agent.py:23 prompt (method-code)\",\n",
      "  \"/Users/jmn/Projects/gpt-toolbox/src/agents/few_shot/agent.py:14 system_prompt (method-ast)\"\n",
      "]\n",
      "[\n",
      "  \"/Users/jmn/Projects/gpt-toolbox/src/plugin/api/schema.py:35 ShellRequest (class-code)\",\n",
      "  \"/Users/jmn/Projects/gpt-toolbox/src/plugin/api/schema.py:35 ShellRequest (class-ast)\",\n",
      "  \"/Users/jmn/Projects/gpt-toolbox/src/plugin/api/routes/shell.py:7 _shell (function-ast)\"\n",
      "]\n",
      "[\n",
      "  \"/Users/jmn/Projects/gpt-toolbox/src/llm/count_tokens.py:8 count_tokens (function-code)\",\n",
      "  \"/Users/jmn/Projects/gpt-toolbox/src/llm/chat_completion.py:44 chat_completion_token_counts (function-code)\",\n",
      "  \"/Users/jmn/Projects/gpt-toolbox/src/llm/count_tokens.py:8 count_tokens (function-ast)\",\n",
      "  \"/Users/jmn/Projects/gpt-toolbox/src/llm/chat_session.py:37 token_counts (method-code)\",\n",
      "  \"/Users/jmn/Projects/gpt-toolbox/src/llm/count_tokens.py:  (module-ast)\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "def print_results_summary(results):\n",
    "    # using the metadata, we can show where, when, and how the chunk was extracted\n",
    "    lines = [\n",
    "        f\"{result.metadata['file_path']}:{result.metadata['lineno']} \" \n",
    "        f\"{result.metadata['node_name']} ({result.metadata['node_type']}-{result.metadata['output_type']})\"\n",
    "        #f\"last modified:{result.metadata['last_modified_time']} \"\n",
    "        #f\"doc_id:{result._id}\"\n",
    "        for result in results\n",
    "    ]\n",
    "    print(json.dumps(lines, indent=2))\n",
    "\n",
    "print_results_summary(db.query(\"system_prompt\", max_results=3))\n",
    "\n",
    "print_results_summary(db.query('schema/ShellRequest', max_results=3))\n",
    "\n",
    "print_results_summary(db.query(\"count tokens\", max_results=5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🧰 Putting it all together with `PythonRetriever`\n",
    "\n",
    "The \"retriever\" classes wrap the extractor and database operations we just manually did above. This accommodates a simple interface for loading and searching. \n",
    "\n",
    "Here is `PythonRetriever`. It can load chunks of code, files, or entire directories of python, and also provides higher-level convenience methods for searching things like functions and comments. Compare these to the same searches done above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total documents in store: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b09e1e7087ac41218ea0d46f5d217abc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding documents to store:   0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from retrieval import PythonRetriever\n",
    "\n",
    "# create a new database. we could easily re-use the same one as before, but we want to demo index()\n",
    "db = Chroma('python-example-2', \"/Users/jmn/chroma/retrieval-module-doc\")\n",
    "print('total documents in store:', db.collection.count())\n",
    "\n",
    "retriever = PythonRetriever(db)\n",
    "\n",
    "# re-index the same stuff as before, but now through the PythonRetriever interface\n",
    "retriever.load_project(os.path.join(os.getcwd(), \"..\", \"src\", \"plugin\")) # again, our own src/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  \"/Users/jmn/Projects/gpt-toolbox/src/plugin/api/schema.py:35 ShellRequest (class-ast)\",\n",
      "  \"/Users/jmn/Projects/gpt-toolbox/src/plugin/api/schema.py:35 ShellRequest (class-code)\"\n",
      "]\n",
      "[\n",
      "  \"/Users/jmn/Projects/gpt-toolbox/src/plugin/api/routes/shell.py:8  (comment-comment)\",\n",
      "  \"/Users/jmn/Projects/gpt-toolbox/src/plugin/api/routes/search.py:20  (comment-comment)\",\n",
      "  \"/Users/jmn/Projects/gpt-toolbox/src/plugin/api/routes/url.py:13  (comment-comment)\"\n",
      "]\n",
      "[\n",
      "  \"/Users/jmn/Projects/gpt-toolbox/src/plugin/api/schema.py:38 ShellResult (class-code)\",\n",
      "  \"/Users/jmn/Projects/gpt-toolbox/src/plugin/api/schema.py:38 ShellResult (class-ast)\",\n",
      "  \"/Users/jmn/Projects/gpt-toolbox/src/plugin/api/schema.py:61 MemorizeResult (class-code)\"\n",
      "]\n",
      "[\n",
      "  \"/Users/jmn/Projects/gpt-toolbox/src/plugin/api/routes/tasks.py:9 get_all_tasks (method-ast)\",\n",
      "  \"/Users/jmn/Projects/gpt-toolbox/src/plugin/api/schema.py:61 MemorizeResult (class-ast)\",\n",
      "  \"/Users/jmn/Projects/gpt-toolbox/src/plugin/api/routes/tasks.py:46 index (function-ast)\",\n",
      "  \"/Users/jmn/Projects/gpt-toolbox/src/plugin/api/routes/memory.py:32 memory (function-ast)\",\n",
      "  \"/Users/jmn/Projects/gpt-toolbox/src/plugin/api/schema.py:69 RememberResult (class-ast)\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# use the high-level search methods:\n",
    "print_results_summary(retriever.search_for_class(\"ShellRequest\"))\n",
    "\n",
    "print_results_summary(retriever.search_comments(\"schema/ShellRequest\"))\n",
    "\n",
    "print_results_summary(retriever.search_in_file(\"/Users/jmn/Projects/gpt-toolbox/src/plugin/api/schema.py\", \"result\"))\n",
    "\n",
    "# or the basic:\n",
    "print_results_summary(retriever.query(\"count tokens\", max_results=5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
